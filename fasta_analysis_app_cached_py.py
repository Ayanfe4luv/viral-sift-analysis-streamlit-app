# -*- coding: utf-8 -*-
"""fasta_analysis_app_cached.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OHs1yIt_q32jZ0S31zfia_LGwYnJU2B6
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from collections import Counter, defaultdict
import re
import os
import gzip
import zipfile
import requests
import time
import io
import gc # Import garbage collector

# ==================== TRANSLATIONS ====================
# (Translation dictionary remains the same as your provided file)
TRANSLATIONS = {
    "en": {
        "app_title": "ðŸ§¬ FASTA Analysis Tool",
        "upload_tab": "ðŸ“ Upload & Setup",
        "manage_tab": "ðŸ—‚ï¸ Manage Datasets",
        "analyze_tab": "ðŸ”¬ Analyze & Process",
        "refine_tab": "ðŸŽ¯ Refine & Visualize",
        "export_tab": "ðŸ“Š Export & Reports",
        "docs_tab": "ðŸ“– Documentation",
        "file_uploader_label": "Upload FASTA files",
        "url_input_label": "Download from URL",
        "download_url_btn": "Download from URL",
        "convert_headers_btn": "Convert Headers",
        "quality_filter_btn": "Apply Quality Filter",
        "deduplicate_basic_btn": "Deduplicate (Sequence Only)",
        "deduplicate_advanced_btn": "Deduplicate (Sequence + Subtype)",
        "filter_subtype_btn": "Filter by Subtype",
        "check_subtypes_btn": "Check Subtype Distribution",
        "temporal_filter_btn": "Enhanced Temporal Filter", # Name changed slightly for clarity
        "clade_filter_btn": "Apply Clade Monthly Filter", # Name changed slightly for clarity
        "extract_accessions_btn": "Extract EPI_ISL Accessions",
        "export_fasta_btn": "Export FASTA",
        "export_report_btn": "Export Report",
        "no_data_msg": "No data loaded or activated. Please upload/activate data first.",
        "sequences_loaded": "sequences loaded",
        "processing": "Processing...",
        "complete": "Complete!",
        "min_length_label": "Min Sequence Length",
        "max_n_run_label": "Max N-Run Length",
        "subtype_label": "Select Subtype",
        "custom_subtype_placeholder": "e.g., H5N1,H3N2",
        "group_by_label": "Group By",
        "sort_by_label": "Sort By",
        "keep_label": "Keep",
        "metric_title": "Active Sequences",
        "gauge_title": "Avg Sequence Length",
        "distribution_title": "Subtype Distribution",
        "activate_btn": "âœ… Activate Selected Files",
        "merge_btn": "ðŸ”— Merge & Download Selected",
        "remove_btn": "ðŸ—‘ï¸ Remove Selected from Session",
        "lang_selector": "Language",
        "select_all_btn": "Select All",
        "deselect_all_btn": "Deselect All",
        "seqs_abbrev": "seqs",
        "field_label": "Field to Visualize:",
        "chart_type_label": "Chart Type:",
        "generate_chart_btn": "ðŸ“Š Generate Chart",
        "generating_chart": "Generating chart...",
        "chart_ready": "Chart ready!",
        "no_sequences_error": "No sequences loaded or active.",
        "chart_error": "Error generating chart",
        "distribution_viewer_title": "ðŸ“Š Data Visualizer",
        "visualizer_desc": "Explore distributions within the active dataset.",
        "clade_monthly_header": "Clade-Based Monthly Filter",
        "clade_mode_single": "Single Clade",
        "clade_mode_multiple": "Multiple Clades",
        "select_clade": "Select Clade:",
        "select_clades": "Select Clades:",
        "keep_monthly_label": "Keep per Month:",
        "temporal_order_first": "First Only",
        "temporal_order_last": "Last Only",
        "temporal_order_both": "Both (First & Last)",
        "process_clades_separately": "Process each selected clade separately",
        "apply_clade_filter_button": "Apply Clade Monthly Filter",
        "enhanced_temporal_header": "Enhanced Temporal Diversity Filter",
        "temporal_group_location_host_month_clade": "Location+Host+Month+Clade",
        "temporal_group_location": "Location",
        "temporal_group_host": "Host",
        "temporal_group_clade": "Clade",
        "temporal_group_location_host": "Location+Host",
        "temporal_group_host_clade": "Host+Clade",
        "temporal_group_none": "No Grouping",
        "temporal_group_custom": "Custom",
        "temporal_sort_date": "Collection Date",
        "temporal_sort_location": "Location",
        "temporal_sort_host": "Host",
        "temporal_sort_clade": "Clade",
        "temporal_sort_isolate": "Isolate ID",
        "keep_per_group_label": "Keep per Group:",
        "custom_grouping_label": "Custom Grouping Fields (comma-sep):",
        "apply_temporal_filter_button": "Apply Enhanced Temporal Filter",
        "last_report_header": "Last Analysis Report",
        "download_active_button": "â¬‡ï¸ Download Current Active Data",
        "export_logs_header": "Export Logs",
        "download_log_button": "â¬‡ï¸ Download Full Log",
        "show_log_expander": "Show Current Log",
        "docs_header": "ðŸ“– Documentation",
        "footer_text": "FastaFlow - Viral Genome Analysis Toolkit",
         "file_manager_empty_title": "No Files Loaded Yet",
        "file_manager_empty_subtitle": "Upload FASTA files via the methods above.",
        "step1_title": "Upload Files:",
        "step1_desc": "Use the upload options to load FASTA data.",
        "step2_title": "Manage Datasets:",
        "step2_desc": "Loaded files appear here. Check files to work with.",
        "step3_title": "Activate:",
        "step3_desc": "Click 'Activate Selected' to load data for analysis.",
        "step4_title": "Analyze:",
        "step4_desc": "Go to other tabs (Analyze, Refine) to process active data.",
        "tip_title": "Pro Tip:",
        "tip_multi_file": "Load multiple files and activate specific subsets for analysis!"
    },
    "ru": {
        # --- Add Russian translations here ---
        "app_title": "ðŸ§¬ Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ ÐÐ½Ð°Ð»Ð¸Ð·Ð° FASTA",
        "upload_tab": "ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°",
        "manage_tab": "ðŸ—‚ï¸ Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐÐ°Ð±Ð¾Ñ€Ð°Ð¼Ð¸",
        "analyze_tab": "ðŸ”¬ ÐÐ½Ð°Ð»Ð¸Ð· Ð¸ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°",
        "refine_tab": "ðŸŽ¯ Ð£Ñ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ð¸ Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ",
        "export_tab": "ðŸ“Š Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð¸ ÐžÑ‚Ñ‡ÐµÑ‚Ñ‹",
        "docs_tab": "ðŸ“– Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ",
        "file_uploader_label": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»Ñ‹ FASTA",
        "url_input_label": "Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¿Ð¾ URL",
        "download_url_btn": "Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ð¿Ð¾ URL",
        "convert_headers_btn": "ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸",
        "quality_filter_btn": "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð°",
        "deduplicate_basic_btn": "Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ (Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ)",
        "deduplicate_advanced_btn": "Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ (ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ + ÐŸÐ¾Ð´Ñ‚Ð¸Ð¿)",
        "filter_subtype_btn": "Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÐŸÐ¾Ð´Ñ‚Ð¸Ð¿Ñƒ",
        "check_subtypes_btn": "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐŸÐ¾Ð´Ñ‚Ð¸Ð¿Ð¾Ð²",
        "temporal_filter_btn": "Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€",
        "clade_filter_btn": "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ ÐšÐ»Ð°Ð´Ñ‹",
        "extract_accessions_btn": "Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ EPI_ISL ÐÐ¾Ð¼ÐµÑ€Ð°",
        "export_fasta_btn": "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ FASTA",
        "export_report_btn": "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ ÐžÑ‚Ñ‡ÐµÑ‚Ð°",
        "no_data_msg": "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸Ð»Ð¸ Ð½Ðµ Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹. Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ/Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.",
        "sequences_loaded": "Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾",
        "processing": "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°...",
        "complete": "Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!",
        "min_length_label": "ÐœÐ¸Ð½. Ð”Ð»Ð¸Ð½Ð° ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸",
        "max_n_run_label": "ÐœÐ°ÐºÑ. Ð”Ð»Ð¸Ð½Ð° N-Ð¡ÐµÑ€Ð¸Ð¸",
        "subtype_label": "Ð’Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ ÐŸÐ¾Ð´Ñ‚Ð¸Ð¿",
        "custom_subtype_placeholder": "Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, H5N1,H3N2",
        "group_by_label": "Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾",
        "sort_by_label": "Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾",
        "keep_label": "ÐžÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ",
        "metric_title": "ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸",
        "gauge_title": "Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð”Ð»Ð¸Ð½Ð° ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸",
        "distribution_title": "Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐŸÐ¾Ð´Ñ‚Ð¸Ð¿Ð¾Ð²",
        "activate_btn": "âœ… ÐÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð’Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ",
        "merge_btn": "ðŸ”— ÐžÐ±ÑŠÐµÐ´Ð¸Ð½Ð¸Ñ‚ÑŒ Ð¸ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ",
        "remove_btn": "ðŸ—‘ï¸ Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð’Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ",
        "lang_selector": "Ð¯Ð·Ñ‹Ðº",
        "select_all_btn": "Ð’Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð’ÑÐµ",
        "deselect_all_btn": "Ð¡Ð½ÑÑ‚ÑŒ Ð’ÑÐµ",
        "seqs_abbrev": "Ð¿Ð¾ÑÐ».",
        "field_label": "ÐŸÐ¾Ð»Ðµ Ð´Ð»Ñ Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸:",
        "chart_type_label": "Ð¢Ð¸Ð¿ Ð”Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñ‹:",
        "generate_chart_btn": "ðŸ“Š Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð”Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ",
        "generating_chart": "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñ‹...",
        "chart_ready": "Ð”Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð°!",
        "no_sequences_error": "ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð½Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸Ð»Ð¸ Ð½Ðµ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹.",
        "chart_error": "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñ‹",
        "distribution_viewer_title": "ðŸ“Š Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð”Ð°Ð½Ð½Ñ‹Ñ…",
        "visualizer_desc": "Ð˜ÑÑÐ»ÐµÐ´ÑƒÐ¹Ñ‚Ðµ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð² Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¼ Ð½Ð°Ð±Ð¾Ñ€Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ….",
        "clade_monthly_header": "Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÐšÐ»Ð°Ð´Ð°Ð¼ Ð¸ ÐœÐµÑÑÑ†Ð°Ð¼",
        "clade_mode_single": "ÐžÐ´Ð½Ð° ÐšÐ»Ð°Ð´Ð°",
        "clade_mode_multiple": "ÐÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÐšÐ»Ð°Ð´",
        "select_clade": "Ð’Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ ÐšÐ»Ð°Ð´Ñƒ:",
        "select_clades": "Ð’Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ ÐšÐ»Ð°Ð´Ñ‹:",
        "keep_monthly_label": "ÐžÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð² ÐœÐµÑÑÑ†:",
        "temporal_order_first": "Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐŸÐµÑ€Ð²ÑƒÑŽ",
        "temporal_order_last": "Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑŽÑŽ",
        "temporal_order_both": "ÐžÐ±Ðµ (ÐŸÐµÑ€Ð²ÑƒÑŽ Ð¸ ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑŽÑŽ)",
        "process_clades_separately": "ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ ÐºÐ°Ð¶Ð´ÑƒÑŽ ÐºÐ»Ð°Ð´Ñƒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾",
        "apply_clade_filter_button": "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ ÐšÐ»Ð°Ð´ Ð¸ ÐœÐµÑÑÑ†ÐµÐ²",
        "enhanced_temporal_header": "Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€",
        "temporal_group_location_host_month_clade": "ÐœÐµÑÑ‚Ð¾+Ð¥Ð¾Ð·ÑÐ¸Ð½+ÐœÐµÑÑÑ†+ÐšÐ»Ð°Ð´Ð°",
        "temporal_group_location": "ÐœÐµÑÑ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ",
        "temporal_group_host": "Ð¥Ð¾Ð·ÑÐ¸Ð½",
        "temporal_group_clade": "ÐšÐ»Ð°Ð´Ð°",
        "temporal_group_location_host": "ÐœÐµÑÑ‚Ð¾+Ð¥Ð¾Ð·ÑÐ¸Ð½",
        "temporal_group_host_clade": "Ð¥Ð¾Ð·ÑÐ¸Ð½+ÐšÐ»Ð°Ð´Ð°",
        "temporal_group_none": "Ð‘ÐµÐ· Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸",
        "temporal_group_custom": "ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹",
        "temporal_sort_date": "Ð”Ð°Ñ‚Ð° Ð¡Ð±Ð¾Ñ€Ð°",
        "temporal_sort_location": "ÐœÐµÑÑ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ",
        "temporal_sort_host": "Ð¥Ð¾Ð·ÑÐ¸Ð½",
        "temporal_sort_clade": "ÐšÐ»Ð°Ð´Ð°",
        "temporal_sort_isolate": "ID Ð˜Ð·Ð¾Ð»ÑÑ‚Ð°",
        "keep_per_group_label": "ÐžÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð² Ð“Ñ€ÑƒÐ¿Ð¿Ðµ:",
        "custom_grouping_label": "ÐŸÐ¾Ð»Ñ Ð´Ð»Ñ Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸ (Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ):",
        "apply_temporal_filter_button": "ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€",
        "last_report_header": "ÐŸÐ¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÐžÑ‚Ñ‡ÐµÑ‚ ÐÐ½Ð°Ð»Ð¸Ð·Ð°",
        "download_active_button": "â¬‡ï¸ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð”Ð°Ð½Ð½Ñ‹Ðµ",
        "export_logs_header": "Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð›Ð¾Ð³Ð¾Ð²",
        "download_log_button": "â¬‡ï¸ Ð¡ÐºÐ°Ñ‡Ð°Ñ‚ÑŒ ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð›Ð¾Ð³",
        "show_log_expander": "ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¢ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð›Ð¾Ð³",
        "docs_header": "ðŸ“– Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ",
        "footer_text": "FastaFlow - Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ ÐÐ½Ð°Ð»Ð¸Ð·Ð° Ð’Ð¸Ñ€ÑƒÑÐ½Ñ‹Ñ… Ð“ÐµÐ½Ð¾Ð¼Ð¾Ð²",
        "file_manager_empty_title": "Ð¤Ð°Ð¹Ð»Ñ‹ Ð•Ñ‰Ðµ ÐÐµ Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹",
        "file_manager_empty_subtitle": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ FASTA, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð²Ñ‹ÑˆÐµ.",
        "step1_title": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¤Ð°Ð¹Ð»Ñ‹:",
        "step1_desc": "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¾Ð¿Ñ†Ð¸Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð»Ñ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… FASTA.",
        "step2_title": "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐÐ°Ð±Ð¾Ñ€Ð°Ð¼Ð¸:",
        "step2_desc": "Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¿Ð¾ÑÐ²ÑÑ‚ÑÑ Ð·Ð´ÐµÑÑŒ. ÐžÑ‚Ð¼ÐµÑ‚ÑŒÑ‚Ðµ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ.",
        "step3_title": "ÐÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ:",
        "step3_desc": "ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ 'ÐÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð’Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ' Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð°Ð½Ð°Ð»Ð¸Ð·.",
        "step4_title": "ÐÐ½Ð°Ð»Ð¸Ð·:",
        "step4_desc": "ÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð²ÐºÐ»Ð°Ð´ÐºÐ¸ (ÐÐ½Ð°Ð»Ð¸Ð·, Ð£Ñ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ) Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ….",
        "tip_title": "Ð¡Ð¾Ð²ÐµÑ‚:",
        "tip_multi_file": "Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°Ð¹Ñ‚Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¿Ð¾Ð´Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°!"
    }
}

DATE_FORMATS = ["%Y-%m-%d", "%d.%m.%Y", "%Y/%m/%d", "%Y-%m", "%Y", "%d-%b-%Y", "%b-%d-%Y", "%Y%m%d"]
DEFAULT_UNKNOWN = "Unknown" # Consistent default value

# ==================== HELPER FUNCTIONS ====================
def get_translation(key, lang=st.session_state.get('lang', 'en')):
    """Get translated text for a key"""
    return TRANSLATIONS.get(lang, TRANSLATIONS["en"]).get(key, f"_{key}_") # Added fallback

def parse_date(date_str):
    """Parse various date formats"""
    if isinstance(date_str, datetime):
        return date_str
    if not date_str or 'unknown' in str(date_str).lower() or date_str is None: # Added None check
        return None
    date_str = str(date_str).strip()
    for fmt in DATE_FORMATS:
        try:
            # Handle partial dates like YYYY or YYYY-MM
            parsed_date = datetime.strptime(date_str, fmt)
            if fmt == "%Y":
                return datetime(parsed_date.year, 1, 1)
            if fmt == "%Y-%m":
                return datetime(parsed_date.year, parsed_date.month, 1)
            return parsed_date
        except ValueError:
            continue
    return None

def update_status(message_key, status_type="info", log=True):
    """Display status message and optionally log"""
    message = get_translation(message_key)
    timestamp = datetime.now().strftime('%H:%M:%S')
    log_entry = f"[{timestamp}] {status_type.upper()}: {message}"

    st.session_state.status_message = message_key # Store the key
    st.session_state.status_level = status_type

    if log and log_entry not in st.session_state.get('analysis_log', []):
        st.session_state.analysis_log.append(log_entry)

    # Display using Streamlit elements in placeholder
    if 'status_placeholder' in st.session_state:
         # Use container to ensure replacement happens correctly
        with st.session_state.status_placeholder.container():
            if status_type == "success":
                st.success(message, icon="âœ…")
            elif status_type == "error":
                st.error(message, icon="âŒ")
            elif status_type == "warning":
                st.warning(message, icon="âš ï¸")
            else: # info
                st.info(message, icon="â„¹ï¸")

# ==================== CORE CLASSES ====================
class ProgressTracker:
    """Simplified tracker using Streamlit session state and update_status."""
    def log(self, message, status_key=None, level='info'):
        update_status_key = status_key if status_key else st.session_state.get('status_message', 'status_ready')
        update_status(update_status_key, level=level, log=True)

    def start_operation(self, operation_name):
        st.session_state.start_time = time.time()
        # Use a generic processing message key
        update_status("processing", level='info', log=True)
        # Add specific operation start to log
        if 'analysis_log' in st.session_state:
             st.session_state.analysis_log.append(f"[{datetime.now().strftime('%H:%M:%S')}] INFO: Started: {operation_name}")


    def complete_operation(self, operation_name, status_key="complete"):
        duration_str = ""
        start_time = st.session_state.pop('start_time', None) # Pop to reset timer
        if start_time:
            duration = time.time() - start_time
            duration_str = f" (Duration: {duration:.2f}s)"

        log_message = f"Completed: {operation_name}{duration_str}"
        level = 'success' if status_key == 'complete' else ('warning' if status_key == 'warning' else 'info')

        # Log completion specifically
        if 'analysis_log' in st.session_state:
             st.session_state.analysis_log.append(f"[{datetime.now().strftime('%H:%M:%S')}] {level.upper()}: {log_message}")

        # Update the main status bar
        update_status(status_key, level=level, log=False) # Don't double-log generic status

    def log_error(self, msg):
        update_status(msg, level='error', log=True) # Log the specific error message


# Instantiate tracker globally
progress_tracker = ProgressTracker()

# --- Cached Parsing Function ---
@st.cache_data # Cache the results based on content_string
def parse_fasta_content(content_string):
    """
    Parses FASTA content string (cached).
    Returns (sequences, errors). Needs FastaParser instance logic.
    """
    # Create a temporary parser instance INSIDE the cached function
    # to access helper methods like _parse_header.
    # This avoids caching the instance itself.
    temp_parser = FastaParser()
    sequences = []
    errors = []
    header = None
    seq_parts = []
    line_num = 0

    try:
        for line_num, line in enumerate(content_string.splitlines(), start=1):
            line = line.strip()
            if not line: continue

            if line.startswith('>'):
                if header is not None:
                    sequence = "".join(seq_parts).upper().replace(" ", "").replace("-", "")
                    if sequence:
                         metadata = temp_parser._parse_header(header)
                         sequences.append([header, sequence, metadata])
                    else:
                         errors.append(f"Line ~{line_num}: Empty sequence for header '{header}'")
                header = line
                seq_parts = []
            elif header is not None:
                seq_parts.append(line)
            else:
                errors.append(f"Line {line_num}: Sequence data before first header ('>'). Ignoring.")

        if header is not None:
            sequence = "".join(seq_parts).upper().replace(" ", "").replace("-", "")
            if sequence:
                 metadata = temp_parser._parse_header(header)
                 sequences.append([header, sequence, metadata])
            else:
                 errors.append(f"End of file: Empty sequence for header '{header}'")

    except Exception as e:
        errors.append(f"Fatal parsing error around line {line_num}: {str(e)}")

    # Crucially, DO NOT interact with st.session_state or UI elements (like st.warning)
    # directly inside a cached function. Return the errors.
    return sequences, errors


class FastaParser:
    """Parse FASTA files and extract metadata"""
    def __init__(self):
        # Reduced host list for example brevity
        self.known_hosts = {'chicken', 'human', 'swine', 'duck', 'avian', 'environment', 'turkey', 'goose', 'wild bird'}

    def _extract_host_and_location(self, isolate_name):
        """Extract host and location from isolate name"""
        try:
            parts = isolate_name.split('/')
            if len(parts) >= 3: # Expect A/Host/Location/...
                potential_host = parts[1].lower().replace('_', ' ')
                # More robust check
                is_known = any(known in potential_host for known in self.known_hosts)
                if is_known or potential_host == 'unknown':
                    host = parts[1].capitalize()
                    location = parts[2].capitalize()
                    return host, location
                else: # Assume A/Location/ID/Year format
                    location = parts[1].capitalize()
                    return DEFAULT_UNKNOWN, location
            elif len(parts) == 2: # Assume A/Location (less common)
                 return DEFAULT_UNKNOWN, parts[1].capitalize()
        except Exception:
            pass # Keep default Unknown on error
        return DEFAULT_UNKNOWN, DEFAULT_UNKNOWN

    def _parse_header(self, header):
        """Parse FASTA header and extract metadata"""
        clean_header = header.lstrip('>').strip()
        metadata = {
            "original_header": header, "isolate_name": clean_header,
            "type": DEFAULT_UNKNOWN, "segment": DEFAULT_UNKNOWN, "collection_date": None,
            "isolate_id": DEFAULT_UNKNOWN, "clade": DEFAULT_UNKNOWN,
            "host": DEFAULT_UNKNOWN, "location": DEFAULT_UNKNOWN
        }

        # Try Pipe-delimited first (more structured)
        if '|' in clean_header:
            parts = [p.strip() for p in clean_header.split('|')]
            metadata['isolate_name'] = parts[0]
            if len(parts) > 1: metadata['type'] = parts[1] if parts[1] else DEFAULT_UNKNOWN
            if len(parts) > 2: metadata['segment'] = parts[2] if parts[2] else DEFAULT_UNKNOWN
            if len(parts) > 3: metadata['collection_date'] = parse_date(parts[3])
            if len(parts) > 4: metadata['isolate_id'] = parts[4] if parts[4] else DEFAULT_UNKNOWN
            if len(parts) > 5: metadata['clade'] = parts[5] if parts[5] else DEFAULT_UNKNOWN
            # Allow override from parts if present
            host_in_parts = parts[6] if len(parts) > 6 and parts[6] else None
            loc_in_parts = parts[7] if len(parts) > 7 and parts[7] else None

            # Fallback parsing from name if parts are missing/default
            host_from_name, loc_from_name = self._extract_host_and_location(metadata['isolate_name'])
            metadata['host'] = host_in_parts if host_in_parts else host_from_name
            metadata['location'] = loc_in_parts if loc_in_parts else loc_from_name

        # Fallback for GISAID-like or simple slash format
        else:
            gisaid_parts = clean_header.split('|') # Check for EPI ID even without full pipes
            name_part = gisaid_parts[0]
            metadata['isolate_name'] = name_part
            host, location = self._extract_host_and_location(name_part)
            metadata['host'] = host
            metadata['location'] = location

            if len(gisaid_parts) > 1 and gisaid_parts[1].startswith('EPI'):
                 metadata['isolate_id'] = gisaid_parts[1]
            if len(gisaid_parts) > 2: # Date might be 3rd element
                 metadata['collection_date'] = parse_date(gisaid_parts[2])

            # Try inferring type/segment from name
            name_lower = name_part.lower()
            if name_lower.startswith('a/'): metadata['type'] = 'A'
            elif name_lower.startswith('b/'): metadata['type'] = 'B'

            # More specific subtype regex
            match_hxny = re.search(r'/(H\d+N\d+)', name_part, re.IGNORECASE)
            if match_hxny:
                 metadata['type'] = match_hxny.group(1).upper()
            elif '(h' in name_lower and 'n' in name_lower: # Look for (H5N1) etc.
                 match_paren = re.search(r'\((H\d+N\d+)\)', name_part, re.IGNORECASE)
                 if match_paren:
                      metadata['type'] = match_paren.group(1).upper()

            # Simple segment check (can be improved)
            if any(seg in name_lower for seg in ['/ha', '(ha)']): metadata['segment'] = 'HA'
            elif any(seg in name_lower for seg in ['/na', '(na)']): metadata['segment'] = 'NA'
            # Add other segments (MP, NP, NS, PA, PB1, PB2)

        return metadata

    def parse(self, file_content_string):
        """
        Parse FASTA content string using the cached function.
        Handles logging of errors returned by the cached function.
        """
        # Call the cached function
        sequences, errors = parse_fasta_content(file_content_string)

        # Log errors AFTER the cached function returns
        if errors:
            st.warning(f"Parser encountered {len(errors)} issues (see details in log).")
            for err in errors[:5]: # Show first few errors
                log_entry = f"[{datetime.now().strftime('%H:%M:%S')}] PARSER_WARN: {err}"
                if log_entry not in st.session_state.analysis_log:
                    st.session_state.analysis_log.append(log_entry)

        return sequences, errors # Return sequences and errors


class SequenceAnalyzer:
    """Analyze and filter FASTA sequences"""
    def __init__(self, sequences):
        # Work on a copy to avoid modifying original session state unintentionally
        self.sequences = [list(item) for item in sequences]
        self.original_count_for_last_op = len(sequences)

    def _update_state_and_log(self, result_sequences, operation_name, removed_headers=None):
        """Helper to update session state and log results."""
        final_count = len(result_sequences)
        removed_count = self.original_count_for_last_op - final_count

        st.session_state.active_sequences = result_sequences # Update main state
        log_message = f"{operation_name}: Kept {final_count}, Removed {removed_count}"
        progress_tracker.complete_operation(log_message, "complete") # Log completion

        # Basic report string generation
        st.session_state.last_report = (
            f"Operation: {operation_name}\n"
            f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"Initial Count: {self.original_count_for_last_op}\n"
            f"Final Count: {final_count}\n"
            f"Removed: {removed_count}"
        )
        if removed_headers:
             st.session_state.last_report += f"\n\nRemoved Headers (sample):\n" + "\n".join(removed_headers[:5]) + ("\n..." if len(removed_headers) > 5 else "")

        return result_sequences # Return for chaining if needed


    def convert_headers(self):
        """Convert headers to standardized pipe format"""
        operation_name = "Convert Headers"
        progress_tracker.start_operation(operation_name)
        converter = FastaConverter(self.sequences, progress_tracker) # Use FastaConverter
        converted_seqs, errors = converter.run()
        # FastaConverter handles logging completion/errors via progress_tracker
        # We just need to update the state
        st.session_state.active_sequences = converted_seqs
        st.session_state.last_report = ( # Simple report update
            f"Operation: {operation_name}\n"
            f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"Headers Processed: {len(converted_seqs)}\n"
            f"Errors: {len(errors)}"
        )
        return converted_seqs


    def quality_filter(self, min_length=200, max_n_run=100):
        """Filter by sequence quality"""
        operation_name = f"Quality Filter (MinLen={min_length}, MaxN={max_n_run})"
        progress_tracker.start_operation(operation_name)
        filtered = []
        removed_headers = []

        for header, seq, metadata in self.sequences:
            if len(seq) < min_length:
                removed_headers.append(header)
                continue

            n_runs = re.findall(r'N+', seq.upper())
            longest_n_run = max(len(run) for run in n_runs) if n_runs else 0

            if longest_n_run > max_n_run:
                removed_headers.append(header)
                continue

            filtered.append([header, seq, metadata])

        return self._update_state_and_log(filtered, operation_name, removed_headers)


    def deduplicate_basic(self):
        """Remove duplicate sequences based on sequence only."""
        operation_name = "Basic Deduplication (Sequence Only)"
        progress_tracker.start_operation(operation_name)
        seen = set()
        unique = []
        removed_headers = []

        for header, seq, metadata in self.sequences:
            if seq not in seen:
                seen.add(seq)
                unique.append([header, seq, metadata])
            else:
                removed_headers.append(header)

        return self._update_state_and_log(unique, operation_name, removed_headers)

    def deduplicate_advanced(self):
        """Remove duplicates preserving subtype diversity per sequence."""
        operation_name = "Advanced Deduplication (Seq + Subtype)"
        progress_tracker.start_operation(operation_name)
        sequence_groups = defaultdict(list)
        for item in self.sequences:
            sequence_groups[item[1]].append(item) # Group by sequence

        unique = []
        removed_headers = []

        for seq, group in sequence_groups.items():
            if len(group) == 1:
                unique.append(group[0]) # No duplicates for this sequence
            else:
                kept_subtypes_for_seq = set()
                # Keep the first occurrence of each subtype for this identical sequence
                for header, _, metadata in sorted(group, key=lambda x: x[0]): # Sort for consistency
                    subtype = metadata.get('type', DEFAULT_UNKNOWN)
                    if subtype not in kept_subtypes_for_seq:
                        unique.append([header, seq, metadata])
                        kept_subtypes_for_seq.add(subtype)
                    else:
                        removed_headers.append(header)

        return self._update_state_and_log(unique, operation_name, removed_headers)

    def filter_by_subtype(self, target_subtypes):
        """Filter sequences by specific subtypes."""
        if not target_subtypes or 'All' in target_subtypes:
             st.info("Filter not applied: No specific subtype selected or 'All' chosen.")
             return self.sequences # Return unchanged

        target_set = {s.strip().upper() for s in target_subtypes}
        operation_name = f"Filter by Subtype ({', '.join(target_set)})"
        progress_tracker.start_operation(operation_name)
        filtered = []
        removed_headers = []

        for header, seq, metadata in self.sequences:
            seq_type = str(metadata.get('type', '')).strip().upper()
            # Check if any target subtype string is present within the sequence's type string
            if any(target in seq_type for target in target_set if target): # Ensure target is not empty
                filtered.append([header, seq, metadata])
            else:
                removed_headers.append(header)

        return self._update_state_and_log(filtered, operation_name, removed_headers)


    def get_subtype_distribution(self):
        """Get subtype distribution counts."""
        progress_tracker.start_operation("Calculating Subtype Distribution")
        counts = Counter(m.get('type', DEFAULT_UNKNOWN) for _, _, m in self.sequences)
        progress_tracker.complete_operation("Subtype distribution calculated")
        return counts

    def enhanced_temporal_filter(self, group_by="location_host",
                                  sort_by="date", keep_per_group="both", custom_grouping=None):
        """Enhanced temporal diversity filter using Pandas."""
        operation_name = f"Enhanced Temporal Filter (Group={group_by}, Sort={sort_by}, Keep={keep_per_group})"
        progress_tracker.start_operation(operation_name)

        if not self.sequences:
            progress_tracker.log_error("No sequences to filter.")
            return []

        df_data = []
        for i, (header, seq, metadata) in enumerate(self.sequences):
            date_val = metadata.get('collection_date') # Already datetime or None
            row = {
                'index': i, 'header': header, 'seq': seq, 'metadata': metadata,
                'date': date_val,
                'location': metadata.get('location', DEFAULT_UNKNOWN),
                'host': metadata.get('host', DEFAULT_UNKNOWN),
                'clade': metadata.get('clade', DEFAULT_UNKNOWN),
                'isolate_id': metadata.get('isolate_id', DEFAULT_UNKNOWN),
                'month': date_val.month if date_val else -1
            }
            # Add custom fields if provided and valid
            if group_by == 'custom' and custom_grouping:
                for field in custom_grouping:
                    row[field] = metadata.get(field, DEFAULT_UNKNOWN)
            df_data.append(row)

        df = pd.DataFrame(df_data)

        # Drop rows where the sort key is invalid for sorting (mainly date)
        if sort_by == 'date':
            df = df.dropna(subset=['date'])
        if df.empty:
            progress_tracker.log_error("No sequences remaining after removing items without sort key.")
            return []

        # Define grouping keys
        group_keys = []
        if group_by == 'none':
            df['group_key_col'] = 'all'
            group_keys = ['group_key_col']
        elif group_by == 'custom' and custom_grouping:
            valid_custom_grouping = [k for k in custom_grouping if k in df.columns]
            if not valid_custom_grouping:
                progress_tracker.log_error("Custom grouping keys not found.")
                return self.sequences # Return original if keys invalid
            group_keys = valid_custom_grouping
        else:
            group_map = {
                "location_host_month_clade": ['location', 'host', 'month', 'clade'],
                "location": ['location'], "host": ['host'], "clade": ['clade'],
                "location_host": ['location', 'host'], "host_clade": ['host', 'clade'],
            }
            group_keys = group_map.get(group_by, ['location', 'host', 'month', 'clade'])

        # Fill NaNs in grouping columns before grouping
        for key in group_keys:
            if key in df.columns:
                 df[key] = df[key].fillna(DEFAULT_UNKNOWN).astype(str) # Convert to string to ensure hashable
            else:
                 progress_tracker.log_error(f"Grouping key '{key}' not found in data.")
                 return self.sequences # Return original

        # Sort within groups
        sort_col = sort_by if sort_by in df.columns else 'date'
        df = df.sort_values(by=[sort_col, 'index'], ascending=True, na_position='last')

        # Apply grouping and filtering
        if group_by == 'none':
             grouped = [('all', df)] # Treat as one group
        else:
             try:
                 grouped = df.groupby(group_keys, observed=True, dropna=False)
             except Exception as e:
                 progress_tracker.log_error(f"Error during grouping: {e}. Check group keys.")
                 return self.sequences


        filtered_indices = []
        for name, group_df in grouped:
            if group_df.empty: continue
            if keep_per_group == "first":
                filtered_indices.append(group_df.index[0])
            elif keep_per_group == "last":
                filtered_indices.append(group_df.index[-1])
            else: # Both
                filtered_indices.append(group_df.index[0])
                if len(group_df) > 1:
                    filtered_indices.append(group_df.index[-1])

        # Get unique indices and retrieve corresponding rows
        filtered_df = df.loc[list(set(filtered_indices))]

        # Convert back to original format
        final_sequences = [
            [row['header'], row['seq'], row['metadata']]
            for _, row in filtered_df.iterrows()
        ]
        # Calculate removed headers based on the original input to *this method*
        original_headers = {h for h, _, _ in self.sequences}
        final_headers = {h for h, _, _ in final_sequences}
        removed_headers = list(original_headers - final_headers)


        return self._update_state_and_log(final_sequences, operation_name, removed_headers)


    def filter_clade_monthly(self, mode, targets, keep_strategy, separate=True):
        """Handles both single and multiple clade monthly filtering."""
        if not targets:
            progress_tracker.log_error("No target clades specified for filtering.")
            return self.sequences # Return original

        target_clades_set = set(targets)
        target_display = targets[0] if mode == 'single' else f"{len(target_clades_set)} clades"
        operation_name = f"{mode.capitalize()} Clade Monthly Filter ({target_display}, Keep={keep_strategy}, Separate={separate if mode=='multiple' else 'N/A'})"
        progress_tracker.start_operation(operation_name)

        # Initial filter based on selected clades
        sequences_to_process = [s for s in self.sequences if s[2].get('clade') in target_clades_set]

        if not sequences_to_process:
            progress_tracker.log_error(f"No sequences found for the specified clades.")
            # Update state with empty list and log correctly
            return self._update_state_and_log([], operation_name, [h for h,_,_ in self.sequences])


        final_sequences = []
        all_removed_headers_step = []

        if mode == 'single' or not separate:
            # Process all relevant sequences together
            processed, removed_step = self._process_monthly_groups(sequences_to_process, keep_strategy)
            final_sequences.extend(processed)
            all_removed_headers_step.extend(removed_step)
        else: # Multiple and Separate
            for clade in target_clades_set:
                clade_seqs = [s for s in sequences_to_process if s[2].get('clade') == clade]
                if clade_seqs:
                    processed, removed_step = self._process_monthly_groups(clade_seqs, keep_strategy)
                    final_sequences.extend(processed)
                    all_removed_headers_step.extend(removed_step) # Collect removed headers from each group

        # Calculate headers removed overall compared to the input of *this* function
        original_headers = {h for h, _, _ in self.sequences}
        final_headers = {h for h, _, _ in final_sequences}
        removed_headers_overall = list(original_headers - final_headers)

        # Update state and log using the overall difference
        return self._update_state_and_log(final_sequences, operation_name, removed_headers_overall)


    def _process_monthly_groups(self, sequences_in_group, keep_strategy):
        """Helper to process monthly groups, returns kept sequences and removed headers for this group."""
        monthly_groups = defaultdict(list)
        kept_sequences = []
        removed_headers_group = []
        original_headers_group = {h for h,_,_ in sequences_in_group}

        # Group by YYYY-MM or 'Unknown'
        for header, seq, metadata in sequences_in_group:
            date_val = metadata.get('collection_date')
            month_key = date_val.strftime('%Y-%m') if date_val else 'Unknown'
            monthly_groups[month_key].append({'header': header, 'seq': seq, 'metadata': metadata, 'date': date_val})

        # Filter within each month
        for month, items in monthly_groups.items():
            if month == 'Unknown' or len(items) <= 1:
                # Keep all 'Unknown' date items or single-item months
                 kept_sequences.extend([[item['header'], item['seq'], item['metadata']] for item in items])
                 continue

            # Sort items with valid dates first, then by header for stability
            items.sort(key=lambda x: (x['date'] if x['date'] else datetime.max, x['header']))

            kept_this_month_items = []
            if keep_strategy == "First Only":
                kept_this_month_items.append(items[0])
            elif keep_strategy == "Last Only":
                kept_this_month_items.append(items[-1])
            else: # Both (First & Last)
                kept_this_month_items.append(items[0])
                if len(items) > 1:
                    # Avoid adding the same item twice if only one item in month
                    if items[0]['header'] != items[-1]['header']:
                        kept_this_month_items.append(items[-1])

            kept_sequences.extend([[item['header'], item['seq'], item['metadata']] for item in kept_this_month_items])


        # Determine removed headers specifically within this group processing step
        kept_headers_group = {h for h,_,_ in kept_sequences}
        removed_headers_group = list(original_headers_group - kept_headers_group)

        return kept_sequences, removed_headers_group


    def extract_accessions(self):
        """Extract accession numbers"""
        progress_tracker.start_operation("Extracting Accession Numbers")
        accessions = []
        seen_accessions = set()
        for header, seq, metadata in self.sequences:
            # Prioritize isolate_id field
            acc = metadata.get('isolate_id', '').strip()
            if acc and acc != DEFAULT_UNKNOWN and acc.startswith('EPI'): # Be more specific for EPI IDs
                if acc not in seen_accessions:
                    accessions.append(acc)
                    seen_accessions.add(acc)
            else:
                 # Fallback: check original header for EPI ID pattern if not found in metadata
                 if '|' in header:
                     parts = header.split('|')
                     for part in parts:
                         part_strip = part.strip()
                         if part_strip.startswith('EPI_ISL_') and part_strip not in seen_accessions:
                              accessions.append(part_strip)
                              seen_accessions.add(part_strip)
                              break # Assume only one EPI per header

        progress_tracker.complete_operation(f"Found {len(accessions)} unique EPI_ISL accessions")
        return accessions


# ==================== VISUALIZATION FUNCTIONS ====================
# (Keep the updated create_metric_indicator, create_gauge_indicator, create_distribution_chart)
def create_metric_indicator(value, title_key, lang="en"):
    """Create a metric indicator"""
    title = get_translation(title_key, lang)
    fig = go.Figure(go.Indicator(
        mode="number",
        value=value,
        title={'text': title, 'font': {'size': 18}}, # Slightly smaller title
        number={'font': {'size': 40, 'color': '#2563eb'}}, # Make number blue
        domain={'x': [0, 1], 'y': [0, 1]}
    ))
    fig.update_layout(
        height=150,
        margin=dict(l=10, r=10, t=40, b=10), # Adjust margins
        paper_bgcolor='rgba(0,0,0,0)', # Transparent background
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def create_gauge_indicator(value, max_value, title_key, lang="en"):
    """Create a gauge indicator"""
    title = get_translation(title_key, lang)
    # Ensure value doesn't exceed max for display
    display_value = min(value, max_value) if value is not None else 0
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=display_value,
        title={'text': title, 'font': {'size': 18}},
        gauge={'axis': {'range': [0, max_value], 'tickwidth': 1, 'tickcolor': "darkblue"},
               'bar': {'color': "#3b82f6", 'thickness': 0.75}, # Blue bar
               'bgcolor': "white",
               'borderwidth': 2,
               'bordercolor': "#e5e7eb",
               'steps': [
                   {'range': [0, max_value * 0.5], 'color': '#e5e7eb'},
                   {'range': [max_value * 0.5, max_value * 0.8], 'color': '#d1d5db'}],
               'threshold': {'line': {'color': "#ef4444", 'width': 4}, 'thickness': 0.8, 'value': max_value * 0.9}},
        number={'font': {'size': 30}, 'suffix': " bp"} # Add units
    ))
    fig.update_layout(
        height=200,
        margin=dict(l=20, r=20, t=50, b=10),
        paper_bgcolor='rgba(0,0,0,0)',
        font={'color': "#374151"}
    )
    return fig

def create_distribution_chart(data_dict, title_key, lang="en", chart_type='bar'):
    """Create distribution pie or bar charts"""
    if not data_dict:
        # Return an empty figure with a message
        fig = go.Figure()
        fig.update_layout(
            title=f"{get_translation(title_key, lang)} (No Data)",
            xaxis={'visible': False}, yaxis={'visible': False},
            annotations=[{'text': 'No data available', 'xref': 'paper',
                        'yref': 'paper', 'showarrow': False, 'font': {'size': 16}}]
        )
        return fig

    title = get_translation(title_key, lang)
    df = pd.DataFrame(list(data_dict.items()), columns=['Category', 'Count'])

    # Limit categories shown for clarity (e.g., top 15 + 'Other')
    limit = 15
    if len(df) > limit:
        df = df.nlargest(limit, 'Count')
        other_count = sum(count for _, count in data_dict.items()) - df['Count'].sum()
        if other_count > 0:
             # Use pd.concat instead of deprecated append
             df_other = pd.DataFrame([{'Category': 'Other', 'Count': other_count}])
             df = pd.concat([df, df_other], ignore_index=True)


    if chart_type.lower() == 'pie':
        fig = px.pie(df, values='Count', names='Category', title=f"{title}",
                     color_discrete_sequence=px.colors.Pastel1) # Use a nice palette
        fig.update_traces(textposition='inside', textinfo='percent+label', pull=[0.05]*len(df)) # Explode slices slightly
        fig.update_layout(legend_title_text='Categories', showlegend=True)
    else: # Default to bar
        fig = px.bar(df.sort_values('Count', ascending=True), # Sort for better bar chart view
                     y='Category', x='Count', title=f"{title}", text_auto=True,
                     orientation='h', # Horizontal bar chart
                     color='Count', # Color by count
                     color_continuous_scale=px.colors.sequential.Blues) # Blue color scale
        fig.update_layout(yaxis_title=None, xaxis_title="Count", coloraxis_showscale=False)
        fig.update_yaxes(categoryorder='total ascending') # Ensure correct order

    fig.update_layout(
        margin=dict(t=50, b=20, l=20, r=20),
        title_font_size=20,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

# ==================== CUSTOM CSS ====================
def load_custom_css():
    """Load custom CSS for better UI"""
    st.markdown("""
    <style>
        /* Main app styling */
        .main .block-container {
            padding-top: 2rem; /* Add padding at the top */
             padding-bottom: 2rem;
        }
        .main {
             /* Light gradient background */
            background: linear-gradient(180deg, #f0f9ff 0%, #e0f2fe 100%);
        }

        /* Card-style containers using Streamlit's internal structure */
        /* Target sections within expanders or main blocks for card look */
        div[data-testid="stExpander"] div[data-testid="stVerticalBlock"],
        div.stTabs [data-baseweb="tab-panel"] > div[data-testid="stVerticalBlock"] > div:not([data-testid="stExpander"]):not(:has(div[data-testid="stExpander"])){
             background: white;
             padding: 25px;
             border-radius: 12px;
             box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
             margin-bottom: 25px;
             border: 1px solid #e5e7eb; /* Subtle border */
        }

        /* Header styling */
        h1 {
            color: #1e3a8a; /* Dark Blue */
            font-weight: 700;
            text-align: center;
            margin-bottom: 0.5rem;
        }
         h2 { /* Tab Headers */
            color: #1d4ed8; /* Medium Blue */
            border-bottom: 2px solid #60a5fa; /* Light Blue */
            padding-bottom: 8px;
            margin-top: 1rem;
            margin-bottom: 1.5rem;
         }
         h3 { /* Section Headers */
            color: #1e40af; /* Darker Medium Blue */
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            font-weight: 600;
         }
         h4 { /* Sub-section Headers */
              color: #1e40af; /* Darker Medium Blue */
              margin-top: 1.5rem;
              margin-bottom: 1rem;
              font-weight: 600;
         }


        /* Button styling */
        .stButton > button {
            border: none;
            border-radius: 8px;
            padding: 10px 20px;
            font-weight: 600;
            transition: all 0.2s ease-in-out;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.08);
        }
        /* Primary button style */
        .stButton > button[kind="primary"] {
             background: linear-gradient(90deg, #3b82f6 0%, #60a5fa 100%); /* Blue gradient */
             color: white;
        }
        .stButton > button[kind="primary"]:hover {
             box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
             filter: brightness(1.1);
        }
         /* Secondary button style */
        .stButton > button[kind="secondary"] {
             background-color: #f3f4f6; /* Light gray */
             color: #dc2626; /* Red text for remove/delete */
             border: 1px solid #ef4444; /* Red border */
        }
        .stButton > button[kind="secondary"]:hover {
             background-color: #fee2e2; /* Light red on hover */
             border-color: #dc2626;
        }

         /* Standard button style */
        .stButton > button:not([kind="primary"]):not([kind="secondary"]) {
            background-color: #ffffff;
            color: #3b82f6; /* Blue text */
            border: 1px solid #d1d5db; /* Gray border */
        }
        .stButton > button:not([kind="primary"]):not([kind="secondary"]):hover {
            background-color: #f9fafb; /* Lighter gray on hover */
            border-color: #9ca3af;
        }


        /* Success/Info/Warning/Error boxes */
        .stAlert {
            border-radius: 8px;
            border-left-width: 5px; /* Thicker left border */
            padding: 1rem;
            box-shadow: 0 2px 6px rgba(0,0,0,0.06);
        }

        /* File uploader */
        .stFileUploader {
            border: 2px dashed #93c5fd; /* Lighter blue dash */
            border-radius: 10px;
            padding: 25px;
            background: #eff6ff; /* Very light blue */
        }
        .stFileUploader label {
            font-weight: 600;
            color: #1d4ed8;
        }

        /* Metric cards */
        div[data-testid="stMetric"] {
            background-color: #ffffff;
            border: 1px solid #e5e7eb;
            padding: 1.5rem;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }
        div[data-testid="stMetricLabel"] {
            font-weight: 600;
            color: #4b5563; /* Gray */
            font-size: 0.95rem;
        }
        div[data-testid="stMetricValue"] {
            font-size: 2.2rem;
            font-weight: 700;
            color: #1e3a8a; /* Dark Blue */
        }
         div[data-testid="stMetricDelta"] {
             font-size: 0.9rem;
         }


        /* Sidebar styling */
        [data-testid="stSidebar"] {
            background: linear-gradient(180deg, #0c4a6e 0%, #0369a1 100%); /* Darker blue gradient */
            padding: 1rem;
        }
        [data-testid="stSidebar"] h3 {
             color: #e0f2fe; /* Light blue text */
             border-bottom: 1px solid #7dd3fc; /* Light blue border */
        }
         [data-testid="stSidebar"] .stMetric {
             background-color: rgba(255, 255, 255, 0.1);
             border: none;
             box-shadow: none;
         }
         [data-testid="stSidebar"] .stMetricLabel {
              color: #e0f2fe; /* Light blue */
              font-size: 0.9rem;
         }
          [data-testid="stSidebar"] .stMetricValue {
              color: #ffffff; /* White */
              font-size: 1.8rem;
         }
         [data-testid="stSidebar"] .stButton > button {
             background-color: rgba(255, 255, 255, 0.2);
             color: white;
             border: 1px solid rgba(255, 255, 255, 0.4);
         }
          [data-testid="stSidebar"] .stButton > button:hover {
              background-color: rgba(255, 255, 255, 0.3);
              border-color: rgba(255, 255, 255, 0.6);
         }
         [data-testid="stSidebar"] .stSelectbox label {
              color: #e0f2fe;
              font-weight: 600;
         }


        /* Tab styling */
        .stTabs [data-baseweb="tab-list"] {
            gap: 12px;
            background-color: transparent; /* Remove default background */
            border-radius: 0;
            padding: 0;
            box-shadow: none;
            border-bottom: 2px solid #d1d5db; /* Underline for inactive tabs */
            margin-bottom: 1.5rem;
        }
        .stTabs [data-baseweb="tab"] {
            background-color: transparent;
            border-radius: 8px 8px 0 0; /* Rounded top corners */
            padding: 12px 24px;
            font-weight: 600;
            color: #4b5563; /* Gray text */
            border: none;
            border-bottom: 2px solid transparent; /* Prepare for active state border */
            margin-bottom: -2px; /* Overlap border */
            transition: all 0.2s ease;
        }
        .stTabs [data-baseweb="tab"]:hover {
             background-color: #f3f4f6; /* Light gray on hover */
             color: #1d4ed8;
        }
        .stTabs [aria-selected="true"] {
             color: #1d4ed8; /* Blue text for active */
             background-color: transparent;
             border-bottom: 2px solid #1d4ed8; /* Blue underline for active */
             box-shadow: none;
        }

        /* Progress indicators */
        .stProgress > div > div {
            background: linear-gradient(90deg, #3b82f6 0%, #60a5fa 100%);
            border-radius: 8px;
        }

        /* Data frame */
        .stDataFrame {
            border-radius: 10px;
            overflow: hidden; /* Ensures border radius applies */
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            border: 1px solid #e5e7eb;
        }

         /* Expander header styling */
        .stExpander > summary {
             background-color: #f9fafb; /* Light gray background */
             border-radius: 8px;
             padding: 10px 15px;
             font-weight: 600;
             color: #1e3a8a; /* Dark blue text */
             border: 1px solid #e5e7eb;
        }
         .stExpander > summary:hover {
              background-color: #f3f4f6;
         }
         .stExpander > div { /* Content within expander */
              border-top: none; /* Remove top border created by default */
              padding-top: 15px;
         }

         /* Specific Info Boxes inside Expanders */
         .stExpander div[style*="background: #e0f2fe"], /* Blue */
         .stExpander div[style*="background: #fef3c7"], /* Yellow */
         .stExpander div[style*="background: #dcfce7"] { /* Green */
             border: none;
             box-shadow: none;
             padding: 12px;
         }
         .stExpander div[style*="background: #e0f2fe"] p,
         .stExpander div[style*="background: #fef3c7"] p,
         .stExpander div[style*="background: #dcfce7"] p {
             font-size: 0.9rem;
             color: #374151; /* Darker gray */
         }


    </style>
    """, unsafe_allow_html=True)


# ==================== SESSION STATE INITIALIZATION ====================
def init_session_state():
    """Initialize session state variables if they don't exist."""
    defaults = {
        'lang': 'en',
        'all_files': {},          # Stores {filename: [sequences]} for all loaded files
        'active_sequences': [],   # The currently active dataset for analysis
        'original_sequences': [], # Copy of active sequences before last filter (for showing removed)
        'analysis_log': [],       # Log of operations performed
        'processing_step': 0,     # Placeholder, could track multi-step processes
        'status_message': "status_ready", # Key for translation
        'status_level': "info",   # Type for status display
        'active_filenames': []    # List of filenames currently active
    }
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

# ==================== MAIN APP ====================
def main():
    st.set_page_config(
        page_title="FastaFlow - FASTA Analysis",
        page_icon="ðŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Load custom CSS
    load_custom_css()

    # Initialize session state
    init_session_state()

    # Sidebar - Use columns for better layout control
    with st.sidebar:
        # Placeholder for a logo or title image
        st.markdown("<h1 style='text-align: center; color: white;'>ðŸ§¬ FastaFlow</h1>", unsafe_allow_html=True)

        # Language Selector
        lang_options = {'en': "ðŸ‡¬ðŸ‡§ English", 'ru': "ðŸ‡·ðŸ‡º Ð ÑƒÑÑÐºÐ¸Ð¹"}
        selected_lang_code = st.selectbox(
            "ðŸŒ Language / Ð¯Ð·Ñ‹Ðº",
            options=list(lang_options.keys()),
            format_func=lambda code: lang_options[code],
            key='lang', # Use session state key directly
            label_visibility="collapsed" # Hide redundant label
        )
        # T function for easy translation access
        T = lambda key: get_translation(key, st.session_state.lang)

        st.markdown("---")

        # Quick Stats Section
        st.markdown("### ðŸ“Š Quick Stats")
        if st.session_state.all_files:
            st.metric("ðŸ“ Files Loaded", len(st.session_state.all_files))
        else:
            st.caption("No files loaded yet.")

        if st.session_state.active_sequences:
            st.metric("ðŸ§¬ Active Sequences", f"{len(st.session_state.active_sequences):,}")
            try:
                avg_len = sum(len(s[1]) for s in st.session_state.active_sequences) / len(st.session_state.active_sequences)
                st.metric("ðŸ“ Avg Length", f"{int(avg_len):,} bp")
            except ZeroDivisionError:
                st.metric("ðŸ“ Avg Length", "N/A")
        else:
            st.caption("No dataset activated.")

        st.markdown("---")

        # Quick Actions Section
        st.markdown("### âš¡ Quick Actions")
        if st.button("ðŸ”„ Reset All Data", use_container_width=True, key="reset_all_sidebar"):
             # Clear most session state keys, preserving language preference
             preserved_lang = st.session_state.lang
             keys_to_reset = list(st.session_state.keys())
             for key in keys_to_reset:
                  if key not in ['lang', 'status_placeholder']: # Keep lang and placeholder
                      del st.session_state[key]
             init_session_state() # Re-initialize defaults
             st.session_state.lang = preserved_lang # Restore language
             update_status("status_ready", level='info')
             st.success("ðŸ”„ Session Reset!")
             time.sleep(1)
             st.rerun()


        # Quick Export only if data is active
        if st.session_state.active_sequences:
            fasta_str_io = io.StringIO()
            for header, seq, _ in st.session_state.active_sequences:
                h = header if isinstance(header, str) else str(header or '')
                h = h if h.startswith('>') else '>' + h
                s = seq if isinstance(seq, str) else str(seq or '')
                fasta_str_io.write(f"{h}\n{s}\n")

            st.download_button(
                label="ðŸ’¾ Quick Export Active FASTA",
                data=fasta_str_io.getvalue(),
                file_name=f"quick_export_{datetime.now().strftime('%Y%m%d_%H%M')}.fasta",
                mime="text/plain",
                use_container_width=True,
                key="quick_export_sidebar"
            )

        st.markdown("---")
        st.caption(f"FastaFlow v1.0 | {datetime.now().year}")


    # Main Area Title
    st.markdown(f"## {T('app_title')}")

    # Status Bar Placeholder - defined globally now
    st.session_state.status_placeholder = st.empty()
    update_status(st.session_state.status_message, st.session_state.status_level, log=False) # Display initial/current status


    # Define Tabs
    tab_keys = ["upload_tab", "manage_tab", "analyze_tab", "refine_tab", "export_tab", "docs_tab"]
    tabs = st.tabs([T(key) for key in tab_keys])
    tab_map = dict(zip(tab_keys, tabs))

    # ==================== TAB 1: UPLOAD & SETUP ====================
    with tab_map["upload_tab"]:
        st.header(T("upload_tab")) # Use header for better hierarchy

        if not st.session_state.all_files:
             # Enhanced Welcome/Guide Box
             st.markdown(f"""
                <div style='background: linear-gradient(135deg, #e0f2fe 0%, #ccfbf1 100%);
                            color: #0c4a6e; padding: 25px; border-radius: 12px; border-left: 6px solid #0ea5e9;'>
                    <h3 style='color: #0c4a6e; border: none; margin-top: 0;'>ðŸ‘‹ Welcome to FastaFlow!</h3>
                    <p style='font-size: 1.05rem;'>Upload your FASTA files below to start analyzing sequences.</p>
                    <p>Use the tabs above to manage datasets, perform analysis, refine results, and export.</p>
                </div>
            """, unsafe_allow_html=True)
             st.markdown("<br>", unsafe_allow_html=True)


        col1, col2 = st.columns([2, 1])

        with col1:
             st.subheader("ðŸ“¤ Upload Files")
             uploaded_files = st.file_uploader(
                 T("file_uploader_label"),
                 type=['fasta', 'fas', 'fa', 'fna', 'txt', 'gz'],
                 accept_multiple_files=True,
                 help="Supports single or multiple files, including .gz compressed",
                 key="main_file_uploader"
             )

             if uploaded_files:
                  # Use a spinner for better feedback
                  with st.spinner("ðŸ”„ Processing uploaded files..."):
                      progress_bar = st.progress(0, text="Initializing...")
                      parser = FastaParser()
                      newly_loaded_count = 0
                      total_sequences_added = 0
                      has_errors = False

                      for idx, uploaded_file in enumerate(uploaded_files):
                          filename = uploaded_file.name
                          progress_text = f"Processing: {filename}"
                          progress_bar.progress((idx) / len(uploaded_files), text=progress_text)

                          if filename not in st.session_state.all_files:
                              try:
                                  content_bytes = uploaded_file.getvalue()
                                  if filename.lower().endswith('.gz'):
                                      content_string = gzip.decompress(content_bytes).decode('utf-8', errors='replace')
                                  else:
                                      content_string = content_bytes.decode('utf-8', errors='replace')

                                  # Call the cached parsing function
                                  sequences, errors = parse_fasta_content(content_string)

                                  if errors:
                                      has_errors = True
                                      st.warning(f"âš ï¸ Issues parsing {filename} (check log in Export tab). First error: {errors[0]}", icon="âš ï¸")
                                      # Log errors after cached function returns
                                      for err in errors[:5]: # Log first few
                                          log_entry = f"[{datetime.now().strftime('%H:%M:%S')}] PARSER_WARN ({filename}): {err}"
                                          if log_entry not in st.session_state.analysis_log:
                                               st.session_state.analysis_log.append(log_entry)


                                  if sequences:
                                      st.session_state.all_files[filename] = sequences
                                      st.session_state.original_sequences[filename] = sequences # Store original on first load
                                      newly_loaded_count += 1
                                      total_sequences_added += len(sequences)
                                  else:
                                      # Update status only if no sequences found AND no previous errors logged
                                      if not errors:
                                         progress_tracker.log_error(f"No valid sequences found in {filename}")


                              except Exception as e:
                                  progress_tracker.log_error(f"Failed to process {filename}: {str(e)}")
                                  has_errors = True
                          # No else needed: Skip if file already loaded

                      progress_bar.progress(1.0, text="Processing complete!")
                      time.sleep(1) # Let user see "complete"
                      progress_bar.empty() # Remove progress bar

                      if newly_loaded_count > 0:
                          msg_key = "complete" if not has_errors else "warning"
                          update_status(f"Loaded {newly_loaded_count} new files ({total_sequences_added} seqs).", msg_key)
                          st.balloons()
                          # Suggest activating if nothing is active yet
                          if not st.session_state.active_sequences:
                              st.info("ðŸ’¡ Go to 'Manage Datasets' to activate files for analysis.")
                      elif not has_errors:
                          update_status("No new valid files found or all files already loaded.", "warning")

                      # Consider clearing uploader state? Need careful handling with rerun
                      # st.session_state.main_file_uploader = [] # This might trigger rerun incorrectly


        with col2:
             st.subheader("ðŸŒ Download via URL")
             url_input = st.text_input(
                 T("url_input_label"),
                 placeholder="https://...",
                 key="url_downloader_input"
             )
             if st.button(T("download_url_btn"), use_container_width=True, key="url_download_button"):
                  if url_input and url_input.startswith(('http://', 'https://')):
                      with st.spinner(f"â³ Downloading from {url_input[:50]}..."):
                          try:
                              response = requests.get(url_input, timeout=DEFAULT_TIMEOUT, stream=True)
                              response.raise_for_status()

                              # Determine filename
                              content_disp = response.headers.get('content-disposition')
                              if content_disp:
                                   fname_match = re.search(r'filename="?([^"]+)"?', content_disp)
                                   filename = fname_match.group(1) if fname_match else None
                              if not filename:
                                   filename = os.path.basename(urllib.parse.urlparse(url_input).path) or f"download_{int(time.time())}.fasta"

                              # Read content (handle potential gzip based on header/extension)
                              is_gzipped = filename.lower().endswith('.gz') or response.headers.get('content-encoding') == 'gzip'
                              content_bytes = response.content # Read all at once for simplicity

                              if is_gzipped:
                                   content_string = gzip.decompress(content_bytes).decode('utf-8', errors='replace')
                                   filename = filename[:-3] if filename.lower().endswith('.gz') else filename # Adjust filename
                              else:
                                   content_string = content_bytes.decode('utf-8', errors='replace')


                              if content_string:
                                   # Call cached parser
                                   sequences, errors = parse_fasta_content(content_string)

                                   if errors:
                                        st.warning(f"âš ï¸ Issues parsing {filename} from URL (check log). First: {errors[0]}", icon="âš ï¸")
                                        for err in errors[:5]:
                                             log_entry = f"[{datetime.now().strftime('%H:%M:%S')}] PARSER_WARN (URL: {filename}): {err}"
                                             if log_entry not in st.session_state.analysis_log:
                                                  st.session_state.analysis_log.append(log_entry)

                                   if sequences:
                                       st.session_state.all_files[filename] = sequences
                                       st.session_state.original_sequences[filename] = sequences
                                       update_status(f"Downloaded and processed {filename} ({len(sequences)} seqs).", "success" if not errors else "warning")
                                       # Activate automatically?
                                       st.session_state.active_filenames = [filename]
                                       st.session_state.active_sequences = sequences
                                       st.info(f"Activated {filename}. Go to 'Manage Datasets' to change.")
                                   else:
                                       progress_tracker.log_error(f"No valid sequences found in content from URL.")
                              else:
                                  progress_tracker.log_error("Empty content received from URL.")

                          except requests.exceptions.RequestException as e:
                              progress_tracker.log_error(f"HTTP Error: {e}")
                          except Exception as e:
                              progress_tracker.log_error(f"Error processing URL: {e}")
                  else:
                      st.warning("Please enter a valid HTTP/HTTPS URL.")


    # ==================== TAB 2: MANAGE DATASETS ====================
    with tab_map["manage_tab"]:
        st.header(T("manage_tab"))

        if not st.session_state.all_files:
            # Display guide if no files are loaded
            st.markdown(f"""
                <div style='background: #e0f2fe; padding: 25px; border-radius: 12px; border-left: 6px solid #0ea5e9;'>
                    <h3 style='color: #0c4a6e; border: none; margin-top: 0;'>{T('file_manager_empty_title')}</h3>
                    <p>{T('file_manager_empty_subtitle')}</p>
                    <ol style='line-height: 1.8; padding-left: 20px;'>
                        <li><b>{T('step1_title')}</b> {T('step1_desc')}</li>
                        <li><b>{T('step2_title')}</b> {T('step2_desc')}</li>
                        <li><b>{T('step3_title')}</b> {T('step3_desc')}</li>
                        <li><b>{T('step4_title')}</b> {T('step4_desc')}</li>
                    </ol>
                     <p><b>ðŸ’¡ {T('tip_title')}</b> {T('tip_multi_file')}</p>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.subheader("ðŸ“‹ Loaded Datasets")
            st.caption("Select files below to include them in the 'Active Dataset' for analysis.")

            file_selection_states = {}
            cols = st.columns(2) # Display files in 2 columns
            sorted_filenames = sorted(st.session_state.all_files.keys())

            for idx, filename in enumerate(sorted_filenames):
                 sequences = st.session_state.all_files[filename]
                 count = len(sequences)
                 # Use filename as key, default based on active_filenames
                 checkbox_key = f"cb_manage_{filename}"
                 default_checked = filename in st.session_state.active_filenames
                 with cols[idx % 2]:
                      # Use markdown for checkbox for better styling control if needed later
                      is_selected = st.checkbox(
                          f"**{filename}** ({count} {T('seqs_abbrev')})",
                          key=checkbox_key,
                          value=default_checked
                      )
                      file_selection_states[filename] = is_selected

            selected_files_now = [fname for fname, selected in file_selection_states.items() if selected]

            st.markdown("---")
            st.subheader("âš¡ Actions on Selected Files")
            action_cols = st.columns(4)

            with action_cols[0]:
                if st.button(T("select_all_btn"), use_container_width=True, key="manage_select_all"):
                    st.session_state.active_filenames = list(st.session_state.all_files.keys())
                    st.experimental_rerun() # Rerun to update checkboxes visually

            with action_cols[1]:
                if st.button(T("deselect_all_btn"), use_container_width=True, key="manage_deselect_all"):
                    st.session_state.active_filenames = []
                    # Update checkbox states manually or rerun
                    for filename in st.session_state.all_files:
                        st.session_state[f"cb_manage_{filename}"] = False
                    st.experimental_rerun()

            with action_cols[2]:
                if st.button(T("activate_btn"), type="primary", use_container_width=True, key="manage_activate",
                             help="Load selected sequences into the active dataset for analysis."):
                    if not selected_files_now:
                        st.warning("No files selected to activate.")
                    else:
                        st.session_state.active_filenames = selected_files_now
                        # Reset original_sequences for the new active set
                        st.session_state.original_sequences = []
                        st.session_state.active_sequences = []
                        for fname in selected_files_now:
                            # Append copies to avoid modifying all_files
                            current_file_seqs = [list(s) for s in st.session_state.all_files.get(fname, [])]
                            st.session_state.active_sequences.extend(current_file_seqs)
                            st.session_state.original_sequences.extend(current_file_seqs) # Store initial state
                        count = len(st.session_state.active_sequences)
                        update_status(f"Activated {len(selected_files_now)} files ({count} seqs).", level='success')
                        # Rerun might be needed if downstream tabs depend on immediate update
                        st.rerun()


            with action_cols[3]:
                 # Add confirmation for removal
                if st.button(T("remove_btn"), type="secondary", use_container_width=True, key="manage_remove"):
                    if not selected_files_now:
                        st.warning("No files selected to remove.")
                    else:
                        # Use session state for confirmation state
                        st.session_state.confirming_removal = True

                if st.session_state.get("confirming_removal", False):
                    st.warning(f"âš ï¸ Are you sure you want to permanently remove {len(selected_files_now)} selected file(s) from this session?")
                    confirm_cols = st.columns(2)
                    with confirm_cols[0]:
                        if st.button("Yes, Remove Files", type="primary", use_container_width=True):
                            removed_count = 0
                            for filename in selected_files_now:
                                if filename in st.session_state.all_files:
                                    del st.session_state.all_files[filename]
                                    removed_count += 1
                                if filename in st.session_state.original_sequences:
                                     # This key stores dict {filename: sequences} - needs care
                                     if filename in st.session_state.original_sequences:
                                          del st.session_state.original_sequences[filename]
                                if filename in st.session_state.active_filenames:
                                    st.session_state.active_filenames.remove(filename)

                            # Rebuild active sequences if affected files were removed
                            if removed_count > 0:
                                st.session_state.active_sequences = []
                                st.session_state.original_sequences = [] # Reset original as well
                                for fname in st.session_state.active_filenames:
                                    current_file_seqs = [list(s) for s in st.session_state.all_files.get(fname, [])]
                                    st.session_state.active_sequences.extend(current_file_seqs)
                                    st.session_state.original_sequences.extend(current_file_seqs)

                                update_status(f"Removed {removed_count} file(s) from session.", level='warning')
                            st.session_state.confirming_removal = False # Reset confirmation state
                            st.rerun() # Rerun to update the list
                    with confirm_cols[1]:
                        if st.button("Cancel", use_container_width=True):
                            st.session_state.confirming_removal = False
                            st.rerun()

            st.markdown("---")
            st.subheader(T("active_dataset"))
            if st.session_state.active_sequences:
                st.write(f"**Files:** `{', '.join(st.session_state.active_filenames)}`")
                st.write(f"**{T('total_seqs')}:** `{len(st.session_state.active_sequences):,}`")
            else:
                st.info("No dataset is currently active. Select files above and click 'Activate Selected Files'.")

    # ==================== TAB 3: ANALYZE & PROCESS ====================
    with tab_map["analyze_tab"]:
        st.header(T("analyze_tab"))

        if not st.session_state.active_sequences:
             # Nicer "No Data" message
             st.markdown(f"""
                <div style='background: #fffbeb; padding: 25px; border-radius: 12px; border-left: 6px solid #f59e0b;'>
                    <h3 style='color: #92400e; border: none; margin-top: 0;'>âš ï¸ No Active Dataset</h3>
                    <p style='color: #78350f;'>Please activate a dataset in the **{T('manage_tab')}** tab first before running analysis.</p>
                </div>
            """, unsafe_allow_html=True)
        else:
            # Re-instantiate analyzer with potentially modified active_sequences
            analyzer = SequenceAnalyzer(st.session_state.active_sequences)

            # --- Dashboard Metrics ---
            st.subheader("ðŸ“Š Current Dataset Overview")
            col1, col2 = st.columns(2)
            with col1:
                st.plotly_chart(create_metric_indicator(
                    len(analyzer.sequences),
                    "metric_title"
                ), use_container_width=True)
            with col2:
                avg_len = sum(len(s[1]) for s in analyzer.sequences) / len(analyzer.sequences) if analyzer.sequences else 0
                st.plotly_chart(create_gauge_indicator(
                    avg_len,
                    max_value=max(2000, int(avg_len * 1.5)), # Dynamic max based on avg
                    title_key="gauge_title"
                ), use_container_width=True)

            st.markdown("---")

            # --- Data Visualizer ---
            with st.expander("ðŸŽ¨ Data Visualizer", expanded=False):
                st.markdown(f"*{get_translation('visualizer_desc')}*")
                vis_col1, vis_col2 = st.columns([3,2])
                with vis_col1:
                    field_options = {
                         'Subtype': 'type', 'Segment': 'segment', 'Host': 'host',
                         'Location': 'location', 'Clade': 'clade',
                         'Year': 'year', 'Month (YYYY-MM)': 'month' # Changed label for clarity
                    }
                    display_field = st.selectbox(get_translation("field_label"), options=list(field_options.keys()), key="vis_field")
                    selected_field_key = field_options[display_field]

                with vis_col2:
                    chart_type = st.selectbox(get_translation("chart_type_label"), ['Bar', 'Pie'], key="vis_chart_type")

                if st.button(get_translation("generate_chart_btn"), key="vis_generate"):
                    with st.spinner(get_translation("generating_chart")):
                        try:
                            # Logic to extract counts
                            if selected_field_key == 'year':
                                 counts = Counter(str(m['collection_date'].year) for _, _, m in analyzer.sequences if m.get('collection_date'))
                            elif selected_field_key == 'month':
                                 counts = Counter(m['collection_date'].strftime('%Y-%m') for _, _, m in analyzer.sequences if m.get('collection_date'))
                            else:
                                 counts = Counter(str(m.get(selected_field_key, DEFAULT_UNKNOWN)) for _, _, m in analyzer.sequences) # Ensure string keys

                            if counts:
                                 fig = create_distribution_chart(counts, f"{display_field} Distribution", chart_type=chart_type.lower())
                                 st.plotly_chart(fig, use_container_width=True)
                                 st.caption(get_translation("chart_ready"))
                            else:
                                 st.warning(f"No data found for field: {display_field}")
                        except Exception as e:
                             st.error(f"{get_translation('chart_error')}: {e}")
                             progress_tracker.log_error(f"Chart generation failed: {e}")


            st.markdown("---")
            st.subheader("ðŸ”§ Processing Steps")

            # Organize steps into columns or expanders for better layout
            col_proc1, col_proc2 = st.columns(2)

            with col_proc1:
                # --- Basic Operations ---
                st.markdown("#### Basic Operations")
                if st.button(T("convert_headers_btn"), key="analyze_convert", use_container_width=True, help="Standardize headers to pipe format"):
                    with st.spinner("Converting headers..."):
                        # Re-instantiate converter with current state
                        converter = FastaConverter(st.session_state.active_sequences, progress_tracker)
                        converted_seqs, errors = converter.run()
                        if not errors: # Only update if successful
                             st.session_state.active_sequences = converted_seqs
                             st.rerun()
                        # Status is updated by the converter method via progress_tracker

                st.markdown("#### Deduplication")
                if st.button(T("deduplicate_basic_btn"), key="analyze_dedup_basic", use_container_width=True, help="Remove identical sequences"):
                     with st.spinner("Running basic deduplication..."):
                         analyzer.remove_duplicates_basic()
                         st.rerun() # Analyzer updates state and logs

                if st.button(T("deduplicate_advanced_btn"), key="analyze_dedup_adv", use_container_width=True, help="Remove identical sequences, keeping one per subtype"):
                     with st.spinner("Running advanced deduplication..."):
                         analyzer.remove_duplicates_preserve_subtypes()
                         st.rerun() # Analyzer updates state and logs


            with col_proc2:
                 # --- Quality Filter ---
                 st.markdown("#### Quality Filter")
                 min_len = st.slider(T("min_length_label"), 0, 3000, 200, 50, key="analyze_min_len", help="Sequences shorter than this will be removed.")
                 max_n = st.slider(T("max_n_run_label"), 0, 500, 100, 10, key="analyze_max_n", help="Sequences with N-runs longer than this will be removed.")
                 if st.button(T("quality_filter_btn"), key="analyze_quality", use_container_width=True):
                     with st.spinner("Applying quality filter..."):
                         analyzer.filter_by_quality(min_length=min_len, max_n_run=max_n)
                         st.rerun() # Analyzer updates state and logs

                 # --- Subtype Operations ---
                 st.markdown("#### Subtype Operations")
                 all_subtypes = ['All'] + sorted(list(set(m.get('type', DEFAULT_UNKNOWN) for _, _, m in st.session_state.active_sequences if m.get('type') != DEFAULT_UNKNOWN)))
                 selected_subtype = st.selectbox(T("subtype_label"), all_subtypes, key="analyze_subtype_select")
                 custom_subtypes_input = st.text_input(f"Or Custom (comma-sep):", placeholder=T("custom_subtype_placeholder"), key="analyze_subtype_custom")

                 sub_op_cols = st.columns(2)
                 with sub_op_cols[0]:
                      if st.button(T("filter_subtype_btn"), key="analyze_subtype_filter", use_container_width=True):
                          targets = []
                          if custom_subtypes_input:
                              targets = [s.strip().upper() for s in custom_subtypes_input.split(',') if s.strip()]
                          elif selected_subtype != 'All':
                              targets = [selected_subtype.upper()]

                          if targets:
                              with st.spinner("Filtering by subtype..."):
                                   analyzer.filter_by_subtype(targets)
                                   st.rerun() # Analyzer updates state and logs
                          else:
                              st.warning("Please select a subtype (or 'All') or enter custom subtypes.")

                 with sub_op_cols[1]:
                      if st.button(T("check_subtypes_btn"), key="analyze_subtype_check", use_container_width=True):
                          dist_counts = analyzer.get_subtype_distribution()
                          if dist_counts:
                              fig = create_distribution_chart(dist_counts, "distribution_title", chart_type='pie')
                              st.plotly_chart(fig, use_container_width=True) # Display chart directly
                          else:
                              st.warning("No subtype information found.")


            # --- Accession Extraction (Moved to Refine tab as per previous structure) ---
            # ... (Consider if it fits better here or in Refine) ...

    # ==================== TAB 4: REFINE & VISUALIZE ====================
    with tab_map["refine_tab"]:
        st.header(T("refine_tab"))

        if not st.session_state.active_sequences:
            st.warning(T("no_data_msg"))
        else:
            analyzer = SequenceAnalyzer(st.session_state.active_sequences)

            # --- Clade Monthly Filter ---
            st.subheader(T("clade_monthly_header"))
            available_clades = sorted([c for c in list(set(m.get('clade', DEFAULT_UNKNOWN) for _, _, m in analyzer.sequences)) if c != DEFAULT_UNKNOWN])

            if not available_clades:
                 st.caption("No clade information available in the active dataset for this filter.")
            else:
                 clade_mode_display = st.radio("Mode", [T("clade_mode_single"), T("clade_mode_multiple")], key="refine_clade_mode", horizontal=True)
                 clade_mode = 'single' if clade_mode_display == T("clade_mode_single") else 'multiple'

                 targets = []
                 separate = True
                 if clade_mode == 'single':
                     target_clade = st.selectbox(T("select_clade"), available_clades, key="refine_clade_single")
                     targets = [target_clade] if target_clade else []
                     separate = False # Not applicable
                 else: # Multiple
                     targets = st.multiselect(T("select_clades"), available_clades, default=available_clades[:1] if available_clades else [], key="refine_clade_multi")
                     separate = st.checkbox(T("process_clades_separately"), True, key="refine_clade_separate")

                 keep_monthly_options = {
                     T("temporal_order_first"): "First Only",
                     T("temporal_order_last"): "Last Only",
                     T("temporal_order_both"): "Both (First & Last)"
                 }
                 keep_monthly_display = st.selectbox(T("keep_monthly_label"), list(keep_monthly_options.keys()), index=2, key="refine_clade_keep")
                 keep_strategy = keep_monthly_options[keep_monthly_display]

                 if st.button(T("apply_clade_filter_button"), key="refine_clade_apply", disabled=not targets):
                     if targets:
                         with st.spinner("Applying clade monthly filter..."):
                              analyzer.filter_clade_monthly(
                                  mode=clade_mode,
                                  targets=targets,
                                  keep_strategy=keep_strategy,
                                  separate=separate
                              )
                              st.rerun() # Analyzer updates state and logs
                     else:
                         st.warning("Please select at least one target clade.")

            st.markdown("---")

            # --- Enhanced Temporal Filter ---
            st.subheader(T("enhanced_temporal_header"))
            group_options = {
                T("temporal_group_location_host_month_clade"): "location_host_month_clade",
                T("temporal_group_location"): "location", T("temporal_group_host"): "host", T("temporal_group_clade"): "clade",
                T("temporal_group_location_host"): "location_host", T("temporal_group_host_clade"): "host_clade",
                T("temporal_group_none"): "none", T("temporal_group_custom"): "custom",
            }
            sort_options = { T("temporal_sort_date"): "date", T("temporal_sort_location"): "location", T("temporal_sort_host"): "host", T("temporal_sort_clade"): "clade", T("temporal_sort_isolate"): "isolate_id", }
            keep_options = { T("temporal_order_first"): "first", T("temporal_order_last"): "last", T("temporal_order_both"): "both", }

            # Use columns for layout
            gsk_cols = st.columns(3)
            with gsk_cols[0]:
                 group_by_display = st.selectbox(T("group_by_label"), list(group_options.keys()), key="refine_temp_group", index=4) # Default to location+host
                 group_by_val = group_options[group_by_display]
            with gsk_cols[1]:
                 sort_by_display = st.selectbox(T("sort_by_label"), list(sort_options.keys()), key="refine_temp_sort")
                 sort_by_val = sort_options[sort_by_display]
            with gsk_cols[2]:
                 keep_per_group_display = st.selectbox(T("keep_per_group_label"), list(keep_options.keys()), index=2, key="refine_temp_keep") # Default to both
                 keep_per_group_val = keep_options[keep_per_group_display]

            custom_grouping_input = ""
            if group_by_val == "custom":
                custom_grouping_input = st.text_input(T("custom_grouping_label"), key="refine_temp_custom", placeholder="e.g., location,host")

            if st.button(T("apply_temporal_filter_button"), key="refine_temp_apply"):
                 custom_grouping_list = [f.strip() for f in custom_grouping_input.split(',')] if group_by_val == "custom" and custom_grouping_input else None
                 with st.spinner("Applying enhanced temporal filter..."):
                      analyzer.enhanced_temporal_diversity_filter(
                          group_by=group_by_val,
                          sort_by=sort_by_val,
                          keep_per_group=keep_per_group_val,
                          custom_grouping=custom_grouping_list
                      )
                      st.rerun() # Analyzer updates state and logs

            st.markdown("---")
            # --- Accession Extraction ---
            st.subheader(T("extract_accessions_btn"))
            if st.button(T("extract_accessions_btn"), key="refine_extract"):
                accessions = analyzer.extract_accessions()
                if accessions:
                    st.session_state.accession_list = accessions # Store for export tab
                    st.success(f"Found {len(accessions)} accessions. Download available in '{T('export_tab')}'.")
                    # Optionally show a preview here
                    st.text_area("Accession Preview (first 20)", "\n".join(accessions[:20]), height=150, disabled=True)
                else:
                    st.warning("No valid EPI_ISL accession numbers found in the current active dataset.")


    # ==================== TAB 5: EXPORT & REPORTS ====================
    with tab_map["export_tab"]:
        st.header(T("export_tab"))

        col_exp1, col_exp2 = st.columns(2)

        with col_exp1:
             st.subheader(T("last_report_header"))
             if st.session_state.last_report:
                 st.text_area("Report Content", value=st.session_state.last_report, height=300, disabled=True, key="export_report_area")
                 st.download_button(
                     label=T("export_report_btn"),
                     data=st.session_state.last_report,
                     file_name=f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                     mime="text/plain",
                     key="export_download_report",
                     use_container_width=True
                 )
             else:
                 st.info("No analysis performed yet in this session to generate a report.")

             # Download current active dataset
             if st.session_state.active_sequences:
                 try:
                     fasta_str_io = io.StringIO()
                     for header, seq, _ in st.session_state.active_sequences:
                         h = header if isinstance(header, str) else str(header or '')
                         h = h if h.startswith('>') else '>' + h
                         s = seq if isinstance(seq, str) else str(seq or '')
                         fasta_str_io.write(f"{h}\n{s}\n")

                     st.download_button(
                          label=f"{T('download_active_button')} ({len(st.session_state.active_sequences)} {T('seqs_abbrev')})",
                          data=fasta_str_io.getvalue(),
                          file_name=f"active_data_{datetime.now().strftime('%Y%m%d_%H%M')}.fasta",
                          mime="text/plain", # Use plain text MIME type
                          key="export_download_active",
                          use_container_width=True,
                          type="primary" # Make it stand out
                     )
                 except Exception as e:
                      st.error(f"Error preparing active data for download: {e}")


             # Download extracted accessions if they exist
             if st.session_state.get('accession_list'):
                  acc_data = "\n".join(st.session_state.accession_list)
                  st.download_button(
                      label=f"â¬‡ï¸ Download Extracted Accessions ({len(st.session_state.accession_list)} IDs)",
                      data=acc_data,
                      file_name=f"extracted_accessions_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                      mime="text/plain",
                      key="export_download_accessions",
                      use_container_width=True
                  )

        with col_exp2:
             st.subheader(T("export_logs_header"))
             log_data = "\n".join(st.session_state.analysis_log)
             st.download_button(
                  label=T("download_log_button"),
                  data=log_data if log_data else "No logs generated in this session.",
                  file_name=f"analysis_log_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                  mime="text/plain",
                  key="export_download_log",
                  use_container_width=True,
                  disabled=not log_data # Disable if no log
             )
             with st.expander(T("show_log_expander")):
                  st.text_area("Log Preview", value=log_data if log_data else "No logs yet.", height=350, disabled=True, key="export_log_preview")


    # ==================== TAB 6: DOCUMENTATION ====================
    with tab_map["docs_tab"]:
        st.header(T("docs_header"))
        # (Documentation Markdown remains the same)
        st.markdown("""
        ## ðŸ§¬ FastaFlow - User Guide

        ### Overview
        This tool provides comprehensive analysis capabilities for influenza and respiratory virus FASTA sequences. Use the tabs to navigate through the workflow: Upload -> Manage -> Analyze -> Refine -> Export.

        ### Features & Guide

        | Feature Tab         | Action                      | Use Case                                                                 | Guide                                                                                                                               |
        | :------------------ | :-------------------------- | :----------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------- |
        | **ðŸ“ Upload & Setup**| File Upload / URL Download | Import sequence data from various sources.                               | Use the upload widget or paste a URL. Supports `.fasta`, `.fa`, `.txt`, `.gz`.                                                     |
        | **ðŸ—‚ï¸ Manage Datasets**| Activate / Remove / Merge   | Work with multiple files, choose subsets for analysis.                 | Check files, click 'Activate Selected'. Use 'Remove' or 'Merge & Download'. Active data is used in Analyze/Refine tabs.              |
        | **ðŸ”¬ Analyze & Process**| Convert Headers             | Standardize headers to `>name|type|...` format.                          | Click 'Convert Headers'. Useful if initial parsing seems incorrect.                                                               |
        |                     | Quality Filter              | Remove low-quality sequences (short or many N's).                        | Adjust sliders for 'Min Length' and 'Max N-Run', then click 'Apply Quality Filter'.                                                |
        |                     | Deduplication (Basic)       | Remove exact sequence duplicates.                                        | Click 'Deduplicate (Sequence Only)'. Keeps the first found instance.                                                              |
        |                     | Deduplication (Advanced)    | Remove duplicates, keeping one per subtype for each unique sequence.   | Click 'Deduplicate (Seq + Subtype)'. Maintains subtype diversity.                                                                   |
        |                     | Subtype Filter              | Isolate sequences of specific subtypes (e.g., H5N1).                     | Select from dropdown or enter custom subtypes (comma-sep), then click 'Apply Subtype Filter'.                                     |
        |                     | Check Subtypes              | Understand subtype proportions in the active dataset.                    | Click 'Check Subtype Distribution'. Displays Pie/Bar charts below.                                                                |
        |                     | Data Visualizer             | Explore distributions (hosts, locations, time, etc.).                    | Select field and chart type (Bar/Pie) in the expander, click 'Generate Chart'.                                                      |
        | **ðŸŽ¯ Refine & Visualize**| Clade Monthly Filter      | Subsample data to get representatives per clade per month.               | Select mode (Single/Multiple), choose clade(s), 'Keep' strategy (First/Last/Both), then click 'Apply'.                            |
        |                     | Enhanced Temporal Filter    | Subsample based on flexible time/metadata grouping.                      | Configure 'Group By', 'Sort By', 'Keep' options, then click 'Apply'. Useful for representative sampling over time/location etc. |
        |                     | Extract Accessions          | Get a list of GISAID EPI_ISL IDs.                                        | Click 'Extract EPI_ISL Accessions'. A download button appears in the **Export** tab.                                               |
        | **ðŸ“Š Export & Reports** | Export FASTA / Report / Log | Download results, reports, and session logs.                           | Click download buttons for the current active FASTA, the last generated report, or the full session log.                              |

        ### Tips
        - **Activation is Key**: Only sequences from *activated* datasets (in the Manage tab) are used for analysis and refinement.
        - **Large Files**: Processing large files can take time. Use the spinners/progress bars as indicators.
        - **Caching**: Parsing is cached; re-uploading the same file content should be faster.
        - **Session Data**: All work is stored in your browser session and will be lost if you close the tab or refresh without uploading again. Use the Export tab to save results.
        """)

    # --- Footer ---
    st.markdown("---")
    st.caption(T("footer_text"))

    # --- Garbage Collection ---
    gc.collect()

if __name__ == "__main__":
    main()
